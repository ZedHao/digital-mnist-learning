# 各种机器学习算法
## 1.LR LogisticRegression
1. 核心：模型训练（学习阶段）
执行 lr.fit(X_train, y_train) 时，模型开始从训练样本中 “学习规律”，核心是通过优化算法找到最优参数（权重和偏置）。这一步是整个过程的 “灵魂”，详细拆解如下：
1. 步骤 1：初始化模型参数（权重 w 和偏置 b）
LR 的 “知识” 存储在权重（w） 和偏置（b） 中：

权重 w：形状为 (10, 784)（10 个类别 ×784 个特征），w[k][j] 表示 “第 j 个像素对判断是否为类别 k 的重要性”。
偏置 b：形状为 (10,)，b[k] 是类别 k 的 “基础得分”。

初始化时，w 和 b 通常被设为 0 或很小的随机值（不同 solver 初始化方式可能不同）。
2. 步骤 2：前向计算（预测概率）
对每个训练样本，模型会计算其属于每个类别的概率，过程如下：

计算线性得分（z）：
对样本 x_i（784 维）和类别 k，计算：
z_i[k] = w[k] · x_i + b[k]（向量点积，即 w[k][0]*x_i[0] + ... + w[k][783]*x_i[783] + b[k]）
含义：z_i[k] 越大，样本 x_i 越可能是类别 k。
转换为概率（p）：
用 Softmax 函数将 z_i 转换为概率（0-1 之间，总和为 1）：
p_i[k] = exp(z_i[k]) / sum(exp(z_i[0..9]))
含义：p_i[k] 是样本 x_i 属于类别 k 的概率。
3. 步骤 3：计算损失（衡量预测误差）
模型通过 “损失函数” 量化预测值与真实标签的差距，目标是最小化损失。

交叉熵损失（核心）：
对单个样本 x_i（真实标签为 y_i），损失为：
loss_i = -log(p_i[y_i])
含义：如果模型预测正确（p_i[y_i] 接近 1），loss_i 接近 0；预测错误则 loss_i 很大。
总损失（加正则化）：
所有样本的平均损失 + 正则化项（防止过拟合）：
total_loss = (1/N) * sum(loss_i) + (λ/2) * sum(w[k][j]^2)
其中 λ = 1/C（C 是初始化时的超参数），正则化项惩罚过大的权重 w。
4. 步骤 4：反向传播（计算梯度）
为了减小损失，需要计算 “每个参数（w 和 b）对损失的影响”，即梯度（偏导数）：

对权重 w[k][j] 的梯度：∂(total_loss)/∂(w[k][j])
对偏置 b[k] 的梯度：∂(total_loss)/∂(b[k])

梯度的意义：

若梯度为正：增大该参数，损失会增加；
若梯度为负：增大该参数，损失会减小。
5. 步骤 5：更新参数（优化算法的作用）
根据梯度，沿 “损失减小的方向” 调整参数（w 和 b），这一步由 solver（如 lbfgs）完成：

权重更新：w[k][j] = w[k][j] - α * 梯度_w（α 是学习率，控制更新幅度）
偏置更新：b[k] = b[k] - α * 梯度_b
6. 步骤 6：迭代收敛（重复步骤 2-5）
不断重复 “前向计算→损失计算→梯度计算→参数更新”，直到：

损失 total_loss 不再明显下降（变化量小于某个阈值）；
达到最大迭代次数（max_iter=1000）。

此时的 w 和 b 就是模型从训练数据中 “学到的规律”—— 比如 “数字 5 的顶部横杠位置像素权重较高”。

7. 四、模型评估与超参数调优（优化阶段）
训练完成后，需要验证模型性能，并可能调整超参数（如 C 的值）以获得更好的模型：

用验证集评估：通过 lr.score(X_val, y_val) 计算验证集准确率，判断模型是否过拟合 / 欠拟合。
网格搜索调优：如之前的 GridSearchCV，测试不同 C 值（如 0.02、0.1、0.2），选择验证集性能最好的超参数组合。
重新训练最优模型：用最优超参数在全量训练数据上重新训练，得到最终模型。
8. 保存模型为.pkl 文件（固化阶段）
训练好的模型（包含最优参数 w、b 和超参数配置）需要保存为 .pkl 文件，方便后续复用：


# 保存模型
    dump(lr, 'lr_model.pkl')
    
    
    .pkl 文件里包含什么？
    模型参数：训练好的权重 w 和偏置 b（核心，模型的 “知识”）。
    超参数配置：penalty、C、solver 等（确保加载后模型结构一致）。
    其他元数据：如标签类别（0-9）、特征维度（784）等，确保预测时输入格式正确。
![img_11.png](img_11.png)
1. 理解L1&L2 [L1去稀疏， L2防止过拟合]
   1. ![img_7.png](img_7.png)
   2. L1正则化和L2正则化可以看做是。损失函数的惩罚项。所谓惩罚是指对损失函数中的某些参数做一些限制
      1. L1正则化是指权值向量w ww中各个元素的绝对值之和
      2. L1正则化 稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵【大部分为0的值没有参考意义】？
      3. https://zhuanlan.zhihu.com/p/343204033
      4. ![img_8.png](img_8.png) ![img_9.png](img_9.png)
   3. L2正则化是指权值向量w ww中各个元素的平方和然后再求平方根
      1. 具体来说，L2正则化在模型优化的过程中，会在损失函数中增加对模型权重平方的正则化惩罚项，
      2. 使得模型在训练时更加关注权重的大小，并将较大的权重进行惩罚，从而降低了过拟合的风险。


# 1. 机器学习
## 1.什么是机器学习
https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247559334&idx=1&sn=ea98c833bdb665df9a89d57be3873493&chksm=fb3b4dadcc4cc4bbbd26d6fbeb706a747059203f8431927a8c1ab43fe94650ccd4e51d1669c3&scene=27

``
1. 假设用性能度量 P 来评估机器在某类任务 T 的性能，若该机器通利用经验 E 在任务 T 中改善其性能 P，那么可以说机器对经验 E 进行了学习。
2. 在该定义中，除了核心词机器和学习，还有关键词经验 E，性能度量 P 和任务 T。在计算机系统中，通常经验 E 是以数据 D 的形式存在，而机器学习就是给定不同的任务 T 从数据中产生模型 M，模型 M 的好坏就用性能度量 P 来评估。 
## 2. 机器学习的四要素
机器学习四要素：数据、任务、性能度量和模型 四个元素：数据 (Data) /任务 (Task)/性能度量 (Quality Metric)/模型 (Model)
### 1.数据
  - 结构化数据和非结构化数据 (按数据具体类型划分)
    - 结构化数据【图片视频等】 
      - 示例 (instance)/特征 (feature) 或输入 (input)/为特征值 (feature value)/标签 (label) 或输出 (output)
      - /样例 (example)= (特征, 标签)/学习 (learning) 或训练 (training)/训练样例 (training example)，整个集合称为训练集 (training set)
    - 非结构化数据【二维的数据表。非结构化数据可以转换成结构化数据】
  - 原始数据和加工数据 (按数据表达形式划分)
  - 样本内数据和样本外数据 (按数据统计性质划分) 样本内用于训练，样本外用来预测
  - 
    ![img_12.png](img_12.png)
    图片处理思路：
    ![img_13.png](img_13.png)
    文本处理思路：
    ![img_14.png](img_14.png)
### 2.任务 (Task)
  - 有监督学习 (有标签)
    - 在有监督学习中，数据 = (特征，标签)，有监督学习 (supervised learning) 利用输入数据及其对应标签来训练模型。 
      - 分类和回归的概念区别
        - 分类【监督学习】：离散值 (discrete value)，例如比赛结果赢或输，此类学习任务称为分类 (classification)。
          - 分类算法：KNN算法、逻辑回归算法、朴素贝叶斯算法、决策树模型、随机森林分类模型、GBDT模型、XGBoost模型、支持向量机模型等。
        - 回归监督学习】：连续值 (continuous value)，例如詹姆斯效率 65.1, 70.3 等等，未来的涨幅，票房房价等，此类学习任务称为回归 (regression)。
          - 决策树模型、随机森林分类模型、GBDT模型、回归树模型、支持向量机模型等。
  - 无监督学习 (无标签)
    - 数据 = (特征，)。，它可以根据电影的各种特征做聚类，用这种方法收集数据为电影推荐系统提供标签。此外无监督学习还可以降低数据的维度，它可以帮助我们更好的理解数据。
    - 聚类
      - 相似兴趣的用户聚集一起
        - ![img_16.png](img_16.png)
        - ![img_17.png](img_17.png)
    - 降维
      - 完整统计数据还有抢断、盖帽和犯规，但这些对预测比赛输赢、效率值都没什么用，因此可以通过降维的方法将其去除。
      - ![img_18.png](img_18.png)
  - 半监督学习 (有部分标签)
  - 增强学习 (有评级标签)
### 3.性能度量 (Quality Metric)
  - 错误率：
  - 查准率和查全率：
    - 错误率衡量了多少比赛实际是赢球但预测成输球。但是若我们关心的是“预测出的比赛中有多少是赢球”，或“赢球的比赛中有多少被预测出了”
  - 混淆矩阵、ROC、AUC
  - 损失函数：
    - ![img_19.png](img_19.png)
    - ![img_20.png](img_20.png)
    - ![img_21.png](img_21.png)
    - ![img_22.png](img_22.png)
### 4.模型 (Model)
- 有监督模型
### 4.1 机器学习模型评估与选择
#### 1. 机器学习与数据拟合
#### 2. 经验误差
![img_24.png](img_24.png)
#### 3. 过拟合
![img_25.png](img_25.png)
   指的是模型在训练集上表现的很好，但是在交叉验证集合测试集上表现一般，也就是说模型对未知样本的预测表现一般，泛化（Generalization）能力较差。
   - 如何防止过拟合：
     - 一般的方法有Early Stopping、数据集扩增（Data Augmentation）、正则化、Dropout等。
     - 正则化：增加数据，简化模型，增加正则项。
#### 4. 偏差
#### 5. 方差
#### 6. 偏差与方差的平衡
![img_23.png](img_23.png)
``
# Sklearn 是什么
``
 和NumPy, SciPy, Pandas, Matplotlib 相似，就是一个处理特殊任务的包，
``
- Sklearn里面有六个任务模块和一个数据引入模块
  - 有监督学习的分类任务 
  - 有监督学习的回归任务 
  - 无监督学习的聚类任务 
  - 无监督学习的降维任务 
  - 数据预处理任务 
  - 模型选择任务
- 数据格式：
  - Numpy 二维数组 (ndarray) 的稠密数据 (dense data)，通常都是这种格式。 
  - SciPy 矩阵 (scipy.sparse.matrix) 的稀疏数据 (sparse data)，比如文本分析每个单词 (字典有 100000 个词 做独热编码得到矩阵有很多 0，这时用 ndarray 就不合适了，太耗内存。
- 核心api
   1. 估计器 (estimator) 拟合器
      1. 有监督学习 model.fit( X_train, y_train )
      2. 无监督学习 model.fit( X_train, y_train )
      3. 拟合之后可以访问 model 里学到的参数，比如线性回归里的特征前的系数 coef_，或 K 均值里聚类标签 labels_。
   2. 预测器 (predictor) 是具有预测功能的估计器 
      1. 转换器也是一种估计器，两者都带拟合功能，估计器里 fit + predict，转换器里 fit + transform。 
         1. 有监督学习的对率回归 预测的类别predict() ，想知道预测该类别的信心predict_proba()。
         2. 无监督学习的K均值
   3. 转换器 (transformer) 是具有转换功能的估计器
      1. 将分类型变量 (categorical) 编码成数值型变量 (numerical) 字符转数字/独热编码
      2. 规范化 (normalize) 或标准化 (standardize) 数值型变量
      3. 警示：fit() 函数只能作用在训练集上，千万不要作用在测试集上，要不然你就犯了数据窥探的错误了！拿标准化举例，用训练集 fit 出来的均值和标准差参数，来对测试集做标准化。
      4. 特征缩放
         1. ![img_4.png](img_4.png)
10. 高级API
    1. 

# 2.GridSearchCV【网格搜索】
1. 人工选择的参数叫超参数，凭经验调或者选不同的参数带入模型选择表现最好的参数
2. 超参数选择不恰当，就会出现欠拟合或者过拟合的问题。
3. 他要求遍历所有可能参数的组合，在面对大数据集和多参数的情况下，非常耗时。
   1. 网格搜索适用于三四个（或者更少）的超参数（当超参数的数量增长时，网格搜索的计算复杂度会呈现指数增长，这时候则使用随机搜索），
4. 理解K折验证[交叉验证]
   1. ![img_10.png](img_10.png)
# 4 .opencv
1. 图像转minst
# 5. tensorflow  
# 6. 梯度下降


 